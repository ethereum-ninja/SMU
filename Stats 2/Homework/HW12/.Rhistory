library(ISLR)
dim(Auto)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
newAuto$cylinders<-factor(newAuto$cylinders)
newAuto$origin<-factor(newAuto$origin)
#
#aggregate is good for summary stats by groups for continous predictors
aggregate(weight~mpg,data=newAuto,summary)
aggregate(displacement~mpg,data=newAuto,summary)
#lets attach newAuto so we don't have to keep writing newAuto$
attach(newAuto)
#Table of counts like proc freq are helpful for categorcal predictors
ftable(addmargins(table(mpg,cylinders)))
#It probably is wise to throw out the 3 and 5 cylinder ones or combine it with
#four or six.  I'll remove to keep it short.
newAuto<-newAuto[-which(cylinders %in% c(3,5)),]
attach(newAuto)
cylinders=factor(cylinders)
levels(cylinders)
ftable(addmargins(table(mpg,origin)))
ftable(addmargins(table(mpg,year)))
#to get proportions that make sense
prop.table(table(mpg,cylinders),2)
prop.table(table(mpg,origin),2)
prop.table(table(mpg,year),2)
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
plot(mpg~origin,col=c("red","blue"))
plot(mpg~year,col=c("red","blue"))
#Visualize
plot(weight~mpg,col=c("red","blue"))
plot(acceleration~mpg,col=c("red","blue"))
plot(displacement~mpg,col=c("red","blue"))
#Examine the correlation between the continous predictors
pairs(newAuto[,3:6])
my.cor<-cor(newAuto[,3:6])
my.cor
#If you have a lot of predictors, heatmap with correlations could
#be helpful to examine redundancy.
library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75),
density.info="none", trace="none", dendrogram=c("row"),
symm=F,symkey=T,symbreaks=T, scale="none")
#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(newAuto[,3:6],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$mpg<-newAuto$mpg
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
geom_point(aes(col=mpg), size=1)+
ggtitle("PCA of Auto")
prop.table(table(mpg,cylinders),2)
table(newAuto$mpg)
##############################
#Interpretation
#The purpose of this code is to illustrate some basic functionality of logistic regression in R.
#Some of the continuous variables look redundant, but for start we will just include everything.
newAuto<-na.omit(newAuto)
model.main<-glm(mpg ~ cylinders+displacement+horsepower+weight+acceleration+year, data=newAuto,family = binomial(link="logit"))
library(ResourceSelection)
library(car)
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we
#discussed in class.
(vif(model.main)[,3])^2
#As expected displacement and the other continuous variables have moderately high VIF.  Based on the pairwise scatterplots
#I believe it is clear.  It is also apparent that cylinders is associated with some of the predictors
#as well. Higher cylinders tends to produce higher horsepower.
#Remember we should not only resort to things like VIF, we should look at the output and
#see if things make sense.
attach(newAuto)
#As expected displacement and the other continuous variables have moderately high VIF.  Based on the pairwise scatterplots
#I believe it is clear.  It is also apparent that cylinders is associated with some of the predictors
#as well. Higher cylinders tends to produce higher horsepower.
#Remember we should not only resort to things like VIF, we should look at the output and
#see if things make sense.
#attach(newAuto)
prop.table(table(mpg,cylinders),2)
t(aggregate(weight~cylinders,data=newAuto,summary))
t(aggregate(weight~cylinders,data=newAuto,summary))
t(aggregate(acceleration~cylinders,data=newAuto,summary))
t(aggregate(horsepower~cylinders,data=newAuto,summary))
t(aggregate(displacement~cylinders,data=newAuto,summary))
#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution.
hoslem.test(model.main$y, fitted(model.main), g=10)
#Summary of current fit
summary(model.main)
#I'm not aware of a nice little automated way to produce Odds ratio metrics
#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
#This is due to the fact that cylinders are correlated with everything.  Go back to EDA and verify.  We just don't
#see the VIF's look too suspect.
t(aggregate(horsepower~cylinders,data=newAuto,summary))
newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
attach(newAuto)
library(ISLR)
dim(Auto)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
newAuto$cylinders<-factor(newAuto$cylinders)
newAuto$origin<-factor(newAuto$origin)
newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
#
#aggregate is good for summary stats by groups for continous predictors
aggregate(weight~mpg,data=newAuto,summary)
aggregate(displacement~mpg,data=newAuto,summary)
#lets attach newAuto so we don't have to keep writing newAuto$
attach(newAuto)
#Table of counts like proc freq are helpful for categorcal predictors
ftable(addmargins(table(mpg,cylinders)))
#It probably is wise to throw out the 3 and 5 cylinder ones or combine it with
#four or six.  I'll remove to keep it short.
newAuto<-newAuto[-which(cylinders %in% c(3,5)),]
attach(newAuto)
cylinders=factor(cylinders)
levels(cylinders)
ftable(addmargins(table(mpg,origin)))
ftable(addmargins(table(mpg,year)))
#to get proportions that make sense
prop.table(table(mpg,cylinders),2)
prop.table(table(mpg,origin),2)
prop.table(table(mpg,year),2)
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
plot(mpg~origin,col=c("red","blue"))
plot(mpg~year,col=c("red","blue"))
#Visualize
plot(weight~mpg,col=c("red","blue"))
plot(acceleration~mpg,col=c("red","blue"))
plot(displacement~mpg,col=c("red","blue"))
#Examine the correlation between the continous predictors
pairs(newAuto[,3:6])
my.cor<-cor(newAuto[,3:6])
my.cor
pairs(newAuto[,3:6],col=mpg)
#If you have a lot of predictors, heatmap with correlations could
#be helpful to examine redundancy.
library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75),
density.info="none", trace="none", dendrogram=c("row"),
symm=F,symkey=T,symbreaks=T, scale="none")
#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(newAuto[,3:6],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$mpg<-newAuto$mpg
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
geom_point(aes(col=mpg), size=1)+
ggtitle("PCA of Auto")
##############################
#Interpretation
#The purpose of this code is to illustrate some basic functionality of logistic regression in R.
#Some of the continuous variables look redundant, but for start we will just include everything.
newAuto<-na.omit(newAuto)
model.main<-glm(mpg ~ cylinders+displacement+horsepower+weight+acceleration+year, data=newAuto,family = binomial(link="logit"))
library(ResourceSelection)
library(car)
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we
#discussed in class.
(vif(model.main)[,3])^2
#As expected displacement and the other continuous variables have moderately high VIF.  Based on the pairwise scatterplots
#I believe it is clear.  It is also apparent that cylinders is associated with some of the predictors
#as well. Higher cylinders tends to produce higher horsepower.
#Remember we should not only resort to things like VIF, we should look at the output and
#see if things make sense.
attach(newAuto)
prop.table(table(mpg,cylinders),2)
t(aggregate(weight~cylinders,data=newAuto,summary))
t(aggregate(acceleration~cylinders,data=newAuto,summary))
t(aggregate(horsepower~cylinders,data=newAuto,summary))
t(aggregate(displacement~cylinders,data=newAuto,summary))
#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution.
hoslem.test(model.main$y, fitted(model.main), g=10)
#Summary of current fit
summary(model.main)
#I'm not aware of a nice little automated way to produce Odds ratio metrics
#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
prop.table(table(mpg,cylinders),2)
