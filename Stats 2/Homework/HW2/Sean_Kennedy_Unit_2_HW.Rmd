---
title: "Unit2 HW"
author: "Turner"
date: ''
output:
  word_document: default
  html_document:
    df_print: paged
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyquant)
library(ISLR)
attach(Auto)
```


## MLR Conceptual questions

  1.  State the necessary assumptions for a multiple linear regression model to be valid in terms of conducting hypothesis tests and providing prediction intervals.
 
  Ideally, the data is:
  
•	normally distributed in their responses across the domain of inputs

•	heteroskedastic (show constant variance)

•	independent within/amongst groups

•	generally exhibit a linear trend with respect to their mean response at each input

  2.  True or False?   It is required that the response variable of an MLR model be normally distributed.
  
  False – statement is too restrictive. Non-normality should lead to further investigation.
 
 
  3.  Feature selection such as forward, backward, stepwise, or penalized techniques provide analysts the ability to do .....what?  For MLR, what can a feature selection method not do without the guidance of the analyst?
  
  Feature selection allows an analyst to permute the number of different possible combinations of explanatory variables. Basically, iteratively adding or removing variables and seeking to minimize/maximize a particular fit criterion, such as ASE or CVPRESS. Feature selection cannot select which criterion is most relevant to your particular problem, nor can it typically discern between variables with actual significance to the problem and those that are not – this is left as an exercise to the user since it typically requires knowledge of the domain. 
  
  4.  Supose that the the true relationship between a response variable ,$y$, and explanatory variable $x$ is $y=\beta_0+\beta_1x+\beta_2x^2+\epsilon$.  If the analysts final model includes a third, fourth, and fifth polynomial term, state whether the test ASE will be higher or lower than the training ASE.  Is this an example of a model having high bias or high variance? 
  
  This model will most likely be overfit due to the addition of the extra terms. In theory – by adding an arbitrary number of extra parameters – one can model any function using a proper choice of coefficients (Taylor expansion all but guarantees this). The overfit model with exhibit very low bias, but parameter estimates will have a wide range of possible values – the extra terms will actually model the random noise term….. not good. For the test/training error rates, the training rate will fall significantly as the model adds more terms while the test rate will begin to suffer due to overfitting.
  


```{r}

library(car)     #where vif function lives
Auto<-Auto[,-9]  #removing the car name
Auto$cylinders = as.factor(Auto$cylinders)
Auto$origin = as.factor(Auto$origin)
full.model<-lm(mpg~.,data=Auto)  # . means all variable not mpg
vif(full.model)
#summary(full.model)
```



__HOMEWORK QUESTION 5.__  

Use this sections EDA output to provide some comments on whether or not multicollinearity may be an issue for this data set.  If so, what variables do you think are redundant and could be removed?  You do not need to do any additional work, but you are welcome to if you want.  
```{r}

cor(select(Auto, -origin, -cylinders)) 

```

horsepower, displacement, weight all seem to be colinear (similar correlations to the target variable from inspection of the correlation matrix)

Running models with each of these variables and excluding the others yields the following:


```{r}


reduced_model_1 = lm(mpg~., data=select(Auto,-acceleration, -displacement, -weight)) #w/ just hp
summary(reduced_model_1)

reduced_model_2 = lm(mpg~., data=select(Auto, -acceleration, -horsepower, -weight)) #w/ just displacement
summary(reduced_model_2)

reduced_model_3 = lm(mpg~., data=select(Auto, -acceleration, -horsepower, -displacement)) #w/ just weight
summary(reduced_model_3)
```

The best of these 3 models is the model with only weight (p-value is very low, and the st error of the estimate is very tight) - probably safe to dump the others and just keep that variable only. Achieves nearly the same predictability as tbe full model (similar R-squared).




__HOMEWORK QUESTION 6.__

#Reanalyze the regression model (produce the same 3 figures from above) using horsepower (and its quadratic term) alone but log transforming mpg. 

```{r, fig.height=2}
par(mfrow=c(1,3))
plot(horsepower,log(mpg), xlab="horsepower",ylab="log(mpg)", col=cylinders)
new<-data.frame(horsepower=seq(30,300,.1))
horse.model3<-lm(log(mpg)~horsepower+I(horsepower^2))
lines(seq(30,300,.1),predict(horse.model3,newdata=new),col="red",lwd=4)
plot(horse.model3$fitted.values,horse.model3$residuals,xlab="Fitted Values",ylab="Residuals", col=cylinders)
plot(horsepower,horse.model3$residuals,xlab="Horsepower",ylab="Residuals", col=cylinders)

```
```{r,fig.height=5,fig.width=5}
par(mfrow=c(2,2))
plot(horse.model3, col=cylinders)
summary(horse.model3)
```

# Does the transformation fix the constant variance issue?  
    
    Yes - variance of residuals seems to be much more consistent throughout the range of log(mpg), varying between -0.5 and 0.5 with few exceptions.
    
#How does normality of the residuals look?  

   QQ plot looks incredibly linear indicating a normally distributed group of residuals.
   

#The plot of log(mpg) vs horsepower looks like the trend may be more linear now and a quadratic term is not needed?  

Let's run a model without the quadratic term

```{r}

par(mfrow=c(1,3))
plot(horsepower,log(mpg), xlab="horsepower",ylab="log(mpg)", col=cylinders)
new<-data.frame(horsepower=seq(30,300,.1))
horse.model4<-lm(log(mpg)~horsepower)
lines(seq(30,300,.1),predict(horse.model4,newdata=new),col="red",lwd=4)
plot(horse.model4$fitted.values,horse.model4$residuals,xlab="Fitted Values",ylab="Residuals", col=cylinders)
plot(horsepower,horse.model4$residuals,xlab="Horsepower",ylab="Residuals", col=cylinders)
summary(horse.model4)

```

#Use residual diagnostics and the  summary function (summary(yourmodel)) to determine if a quadratic term is needed after log transforming.  
Not really - fit diagnostics plots only indicate that the quadratic term is signficant at the higher end of the horsepower range (slight bend upward). While the R-squared value of the model with the quadratic is higher - it's not significantly higher and could be an indication of overfitting in this range.  

##Feature Selection and the Bias-Variance Trade Off##

As discussed during the live session, feature selection is a helpful tool to safeguard our model building processes from building models that are potentially too simple or overly complex.  To get a sense of how to replicate tools we have seen in SAS, lets import the golf data set that we used in our prelive assignment and play around.  (NOTE:  There are more integrated R pacakges for example caret that incorporate other feature selection technques and predictive models.  More integrated often translates to more "black box" and I want to make sure we have a good sense of this critical concept.) 

The leaps packages has a very straightforward tool to use for forward selection. Regsubsets basically works like a general lm regression call with the added options of selecting the type of selection process and the maximum number of steps you want to go.

```{r echo=T, fig.keep = 'none'}
library(leaps)
golf<-read.csv("GolfData2.csv",header=T)
golf<-golf[,-c(1,8,9,10)]
reg.fwd=regsubsets(log(AvgWinnings)~.,data=golf,method="forward",nvmax=20)
```

The object that I created, reg.model, containsShow in New WindowClear OutputExpand/Collapse Output
 
 
mpg
<dbl>
cylinders
<dbl>
displacement
<dbl>
horsepower
<dbl>
weight
<dbl>
acceleration
<dbl>
1	18	8	307	130	3504	12.0	
2	15	8	350	165	3693	11.5	
3	18	8	318	150	3436	11.0	
4	16	8	304	150	3433	12.0	
5	17	8	302	140	3449	10.5	
6	15	8	429	198	4341	10.0	
6 rows | 1-7 of 9 columns
Show in New WindowClear OutputExpand/Collapse Output
      mpg        cylinders  displacement     horsepower        weight    
 Min.   : 9.00   3:  4     Min.   : 68.0   Min.   : 46.0   Min.   :1613  
 1st Qu.:17.00   4:199     1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  
 Median :22.75   5:  3     Median :151.0   Median : 93.5   Median :2804  
 Mean   :23.45   6: 83     Mean   :194.4   Mean   :104.5   Mean   :2978  
 3rd Qu.:29.00   8:103     3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  
 Max.   :46.60             Max.   :455.0   Max.   :230.0   Max.   :5140  
                                                                         
  acceleration        year       origin                  name    
 Min.   : 8.00   Min.   :70.00   1:245   amc matador       :  5  
 1st Qu.:13.78   1st Qu.:73.00   2: 68   ford pinto        :  5  
 Median :15.50   Median :76.00   3: 79   toyota corolla    :  5  
 Mean   :15.54   Mean   :75.98           amc gremlin       :  4  
 3rd Qu.:17.02   3rd Qu.:79.00           amc hornet        :  4  
 Max.   :24.80   Max.   :82.00           chevrolet chevette:  4  
                                         (Other)           :365  
Show in New WindowClear OutputExpand/Collapse Output
            [,1]       [,2]       [,3]       [,4]       [,5]      
cylinders   "3"        "4"        "5"        "6"        "8"       
mpg.Min.    "18.00000" "18.00000" "20.30000" "15.00000" " 9.00000"
mpg.1st Qu. "18.75000" "25.00000" "22.85000" "18.00000" "13.00000"
mpg.Median  "20.25000" "28.40000" "25.40000" "19.00000" "14.00000"
mpg.Mean    "20.55000" "29.28392" "27.36667" "19.97349" "14.96311"
mpg.3rd Qu. "22.05000" "32.95000" "30.90000" "21.00000" "16.00000"
mpg.Max.    "23.70000" "46.60000" "36.40000" "38.00000" "26.60000"
Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output
   cylinders displacement   horsepower       weight acceleration         year 
    1.984867    23.271418    10.559221    11.723047     2.684794     1.323483 
      origin 
    1.531211 
Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output
The working directory was changed to /Volumes/TRAVELDRIVE/MSDS6372/PreLive/Unit2 inside a notebook chunk. The working directory will be reset when the chunk is finished running. Use the knitr root.dir option in the setup chunk to change the working directory for notebook chunks. the results of not only the final regression model but all of the models through the iterative process. This allows us to make graphics similar to the SAS diagnostics of glmselect.

Using the summary command we can extract metrics like ASE, AIC, and BIC for each of the steps.  The vector of results are listed in order of model complexity (1 predictor up to nvmax).  Some examples are below along with a simple visualization which is helpful.  Keep in mind at this point, we are not doing any CV or even training and test set splits.  We just using the entire data set as a training set.

```{r, echo=T, fig.height=3,fig.width=7}
summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic


par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:20,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:20,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:20,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

```

When dealing with only a training data set metrics like AIC and BIC are the most appropriate as they penelize the more complex models.  (See the BIC graph).  Now lets visit the train/test set split for checking model adequacy.

First lets simply split the golf data set into training and split and fit a forward selection model on the training data set.

```{r,echo=T}
set.seed(1234)
index<-sample(1:dim(golf)[1],100,replace=F)
train<-golf[index,]
test<-golf[-index,]
reg.fwd=regsubsets(log(AvgWinnings)~.,data=train,method="forward",nvmax=20)

```

Once we have our model fits on the training data set, all we need to do is predict the models onto the training data set and produce ASE type plots for each step of the forward selection.  Courtesy of the ISLR texbook, a funciton is provided to easily predict the forward selection results on a test set.

```{r, echo=T}
#Really handy predict function
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
```

With the prediction function read in we can simply write a loop to predicted on each of the 20 models generated from the forward selection procedure and plot the ASE's.  I've included the training ASE for comparison.

```{r, echo=T, fig.width=5,fig.height=4}
testASE<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:20){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=test,id=i) 
  testASE[i]<-mean((log(test$AvgWinnings)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:20,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0.3,0.8))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss
lines(1:20,rss/100,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```

From the test ASE graphic, we see (via the red dot) that the minimum Average Squared Error happens with 3 predictors included in the model and its value is around 0.4.  The ASE on the test gives us a metric on how reproducible the models being fitted would be have on a future data set.  It doesn't really help us get at how well the prediction accuracy is though.  To finish things off and after additional model fitting and trial and error, once a model is deemed the final model it is usually standard practice to fit the entire data set once more and make your final reports and conclusions using all of the data (more information to estimate parameters).

```{r, echo=T}
reg.final=regsubsets(log(AvgWinnings)~.,data=golf,method="forward",nvmax=4)
coef(reg.final,3)
final.model<-lm(log(AvgWinnings)~Greens+AvgPutts+Save,data=golf)
summary(final.model)
```

Remember that the output of the final model in terms of reporting p-values and interpretation, you still must check the model assumptions to make sure everything is valid.  As mentioned earlier the ASE plots are really great at helping to select a final model that doesn't have too much bias or variance,  it is hard to tell how well the predictions are.  This depends on the scale of the response variable.  A helpful graphic that SAS produces as well is the true response versus the predictions.  This can give you a sense of the uncertainty in the prediction.  More variability equates to more uncertainty.

```{r,echo=T}
plot(exp(final.model$fitted.values),golf$AvgWinnings,xlab="Predicted",ylab="AvgWinnings",xlim=c(0,400000),ylim=c(0,400000))
lines(c(0,400000),c(0,400000),col="red")
```

Another helpful thing that this graph shows that also would come up in the residual diagnostics is that our predictive modelis going to under predict those extreme golfers who are making more than everybody else.  We would have need some other explantory variable to help capture what is going on out there.

To help with the idea of this graph imagine we fit another predictive model but just used 3 of the random variables.  This should clearly have less predictive ability and we could verify that with a test ASE assessment.  Below is the predictions of the "bogus" model versus the true Avg Winnnings.  We can see the predictions are much worse as they vary much more around the red line.

```{r, echo=F}
bogus.model<-lm(log(AvgWinnings)~V12+V13+V14,data=golf)
plot(exp(bogus.model$fitted.values),golf$AvgWinnings,xlab="Predicted",ylab="AvgWinnings",xlim=c(0,400000),ylim=c(0,400000))
lines(c(0,400000),c(0,400000),col="red")
```


Probably the best way is to make a few predictions and examine and take a look at their prediction intervals.  The tighter the interval the better model you have and you can make some practical sense from there.  Another helpful thing to do would be to take a look at some prediction intervals.  These are on the log scale.  

```{r, echo=T}
head(predict(final.model,golf,interval="predict"))
```

Putting things back on the raw scale, we can see the certainty in our predictions.
For example, the first predicted average winnings for a golfer with Green=58.2, AvgPutts=1.767, and Save=50.9 is $11994.16 with a prediction interval of 3247.77 to 44,294.  The interval is quite large and illustrates just how variable average winnings are even though there are a few key predictors that are statistically relevent and the model producing results that make some sense.



__HOMEWORK QUESTION 7.__

Use the Auto data set and perfrom the following tasks.  Provide your R script for these answers.

#1. Split the data set into a training and test set that is roughly 50/50. Before doing so delete the observations that have a 3 or 5 cylinders.  This will make your life easier.  To keep everyone consistent with each other, make sure you set the seed number "set.seed(1234)" before making the split.  

```{r}
library(sqldf)
set.seed(1234)
autos_no_35 = sqldf('SELECT * FROM Auto WHERE cylinders NOT IN (3, 5)')
index<-sample(1:dim(autos_no_35)[1],100,replace=F)
train<-autos_no_35[index,]
test<-autos_no_35[-index,]
summary(test)
summary(train)
```

#2. By choosing to delete the 3 and 5 cylinder cars from the model how does that change, if anything, about the conclusions one could make from an analysis conducted from this new reduced data set?

```{r}
autos_35 = sqldf('SELECT * FROM Auto WHERE cylinders IN (3, 5)')
train[,3:8]
```

Not much - since we are treating cylinders as a categorical variable

#3. Perfrom a forward selection on the training data set and produce the ASE plot comparing both the training and test set ASE metrics using the following set of predictions: displacement,horsepower,weight,acceleration,year, and origin.  Determine how many predictors should be included.  

```{r}
forward = regsubsets(log(mpg)~displacement+horsepower+weight+acceleration+year+origin, data=train,method="forward",nvmax=7)

testASE<-c()
#note my index is to 7 since that what I set it in regsubsets
for (i in 1:7){
  predictions<-predict.regsubsets(object=forward,newdata=test,id=i) 
  testASE[i]<-mean((log(test$mpg)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:7,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0.01,0.03))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(forward)$rss
lines(1:7,rss/100,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```

Somewhere between 2 and 5 predictors seems reasonable - anythning behond that seems like overfitting


#4. Using your decision from #3, fit a final model using the entire data set and produce the residual diagnostics.  Does your model look reasonable or do you think you should consider some additional iterations?  Give a brief comment.  You do not need to act on the comments.

From prior EDA - it seems like a good model might be a log(mpg) target with a combination of the non-colinear predictors (inlcuding the quadrratic term for HP). 

```{r}
final_model = lm(log(mpg)~horsepower+I(horsepower^2)+year+origin+weight, data=autos_no_35)
summary(final_model)
plot(final_model, col=origin)
```

Not too shabby - residuals look good. Continual iterations run the risk of data snooping.


#5 optional.  



The combinations of origin and cylinder do not match in the test/train split hence they cannot be modeled properly